# datapipeline-redshift-to-rds

This sample demonstrates how you can use Data Pipeline to copy data from a Redshift table to a RDS table. The template assumes that the table is already created in Redshift. To make this sample to work, you must ensure you have the following:

## Prerequisites

Please follow the below tutorial to create the specific roles
* DataPipelineDefaultRole - Grants AWS Data Pipeline access to your AWS resources
* DataPipelineDefaultResourceRole - Grants your applications access to your AWS resources

https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html

* myRDSDatabaseName: Rds Database Instance Name (DB Instance)
* myRedshiftInstanceId: Redshift Cluster name (Cluster)
* myRDSInstanceId: RDS database name
* myRedshiftDatabaseName: Redshift database name
* myRedshiftUsername/myRDSUsername: Redshift username and password.  This user must have write access to the table where data will be copied to.
* myRDSUsername/myRDSPassword: RDS username and password.  This user must have write access to the table where data will be copied to.
* myRdsTableName: RDS table name to be used.
* myRedshiftTableName: Redshift table name to be used.
* myEc2SecurityGrps: Ec2 Security group id so that the resource can access both Redshift and RDS instance.
* myInsertQueryPreparedStatement: RDS Insert prepared statement to be used.
* myCreateTableQuery: RDS create table query statement.
* myOutputS3Path: Temporary S3 location to be used for copying.
* myS3LogsPath: S3 location to direct log messages generated by Data Pipeline.
* mySubnetId: Subnet Id which the staging ec2 will be placed and please ensure the security group belongs to the vpc including this subnet

You will need to provide the above information in the "put-pipeline-definition" command below.

## Getting Started

```sh
 $> aws datapipeline create-pipeline --name redshift_to_rds --unique-id redshift_to_rds

# You receive a pipeline activity like this.
#  {
#      "pipelineId": "df-01556521TW3E885XGBMB"
#  }


#now upload the pipeline definition

 $>  aws datapipeline put-pipeline-definition --pipeline-id df-01556521TW3E885XGBMB 、
     --pipeline-definition \
     file://datapipeline_architect.json --parameter-values \
     myRDSDatabaseName=sample  \
     *myRDSPassword=Abcd1234 \
     myRDSInstanceId=devtest \
     myRDSUsername=admin \
     myRedshiftDatabaseName=sample \
     *myRedshiftPassword=Abcd1234 \
     myRedshiftInstanceId=redshift-cluster \
     myRedshiftUsername=admin \
     myInsertQueryPreparedStatement="insert into sample(col1,col2) values(?,?);" \
     myRedshiftTableName=sample \
     myRDSTableName=sample \
     myEc2SecurityGrps=sg-00b6ad2e965708903 \
     mySubnetId=subnet-064825e056ec802b2 \
     myS3LogsPath=s3://aws-nikofeng-sample-bucket/datapipeline/logs \
     myCreateTableQuery="create table if not exists sample(col1 varchar(255),col2 integer);" \
     myOutputS3Path=s3://aws-nikofeng-sample-bucket/datapipeline

# You receive a validation messages like this

#  {
#      "validationErrors": [],
#      "errored": false,
#      "validationWarnings": []
#  }

#now activate the pipeline
  $> aws datapipeline activate-pipeline --pipeline-id df-032542760T7M3UXU8KJ

