# datapipeline-redshift-to-rds

This sample demonstrates how you can use Data Pipeline to copy data from a Redshift table to a RDS table. The template assumes that the table is already created in Redshift. To make this sample to work, you must ensure you have the following:

## Prerequisites

### 1.Create Data Pipeline IAM roles
* DataPipelineDefaultRole - Grants AWS Data Pipeline access to your AWS resources
* DataPipelineDefaultResourceRole - Grants your applications access to your AWS resources
> https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html

### 2.Configure aws cli
> https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html

## Getting Started

### 1.Create S3 bucket
Firstly you need to create a S3 bucket to store the staging data and log files, please follow below instructions to proceed
> https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html

### 2.Prepare the variables to create data pipeline job
You will need to provide the above information in the "put-pipeline-definition" command below.

Field|Description
---|:---:
myRDSDatabaseName|Rds Database Instance Name (DB Instance)
myRedshiftInstanceId|Redshift Cluster name (Cluster)
myRDSInstanceId|RDS database name
myRedshiftUsername|Redshift cluster master username
myRDSUsername|Rds Database master username
myRedshiftPassword|Redshift cluster master password
myRDSPassword|Rds Database master password
myRdsTableName|RDS table name to be used
myRedshiftTableName|Redshift table name to be used.
myEc2SecurityGrps|Ec2 Security group id so that the resource can access both Redshift and RDS instance.
myInsertQueryPreparedStatement|RDS Insert prepared statement to be used.
myCreateTableQuery|RDS create table query statement.
myOutputS3Path|Temporary S3 location to be used for copying.
myS3LogsPath|S3 location to direct log messages generated by Data Pipeline.
mySubnetId|Subnet Id which the staging ec2 will be placed and please ensure the security group belongs to the vpc including this subnet

### 3.Execute command

```sh
 $> aws datapipeline create-pipeline --name redshift_to_rds --unique-id redshift_to_rds

# You receive a pipeline activity like this.
#  {
#      "pipelineId": "df-01556521TW3E885XGBMB"
#  }


#now upload the pipeline definition

 $>  aws datapipeline put-pipeline-definition --pipeline-id df-01556521TW3E885XGBMB ã€
     --pipeline-definition \
     file://datapipeline_architect.json --parameter-values \
     myRDSDatabaseName=sample  \
     *myRDSPassword=Abcd1234 \
     myRDSInstanceId=devtest \
     myRDSUsername=admin \
     myRedshiftDatabaseName=sample \
     *myRedshiftPassword=Abcd1234 \
     myRedshiftInstanceId=redshift-cluster \
     myRedshiftUsername=admin \
     myInsertQueryPreparedStatement="insert into sample(col1,col2) values(?,?);" \
     myRedshiftTableName=sample \
     myRDSTableName=sample \
     myEc2SecurityGrps=sg-00b6ad2e965708903 \
     mySubnetId=subnet-064825e056ec802b2 \
     myS3LogsPath=s3://aws-nikofeng-sample-bucket/datapipeline/logs \
     myCreateTableQuery="create table if not exists sample(col1 varchar(255),col2 integer);" \
     myOutputS3Path=s3://aws-nikofeng-sample-bucket/datapipeline

# You receive a validation messages like this

#  {
#      "validationErrors": [],
#      "errored": false,
#      "validationWarnings": []
#  }

#now activate the pipeline
  $> aws datapipeline activate-pipeline --pipeline-id df-032542760T7M3UXU8KJ

